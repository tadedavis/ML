{"cells":[{"cell_type":"markdown","metadata":{"id":"hXtuVULYSun5"},"source":["<a \n","href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n","  target=\"_parent\">\n","  <img\n","    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n","    alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"cksgAH12XRjV"},"source":["# Lab 6: Sequence-to-sequence models\n","\n","### Description:\n","For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n","\n","This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n","\n","### Deliverable:\n","- Fill in the code for the RNN (using PyTorch's built-in GRU).\n","- Fill in the training loop\n","- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n","- Implement your own GRU cell.\n","- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n","\n","### Grading Standards:\n","- 20% Implementation the RNN\n","- 20% Implementation training loop\n","- 20% Implementation of evaluation loop\n","- 20% Implementation of your own GRU cell\n","- 20% Training of your RNN on a domain of your choice\n","\n","### Tips:\n","- Read through all the helper functions, run them, and make sure you understand what they are doing\n","- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n","- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n","\n","### Example Output:\n","An example of my final samples are shown below (more detail in the\n","final section of this writeup), after 150 passes through the data.\n","Please generate about 15 samples for each dataset.\n","\n","<code>\n","And ifte thin forgision forward thene over up to a fear not your\n","And freitions, which is great God. Behold these are the loss sub\n","And ache with the Lord hath bloes, which was done to the holy Gr\n","And appeicis arm vinimonahites strong in name, to doth piseling \n","And miniquithers these words, he commanded order not; neither sa\n","And min for many would happine even to the earth, to said unto m\n","And mie first be traditions? Behold, you, because it was a sound\n","And from tike ended the Lamanites had administered, and I say bi\n","</code>\n"]},{"cell_type":"markdown","metadata":{"id":"c2i_QpSsWG4c"},"source":["---\n","\n","## Part 0: Readings, data loading, and high level training\n","\n","---\n","\n","There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n","\n","* Read the following\n","\n","> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (You will be implementing the decoder, not the encoder, as we are not doing sequence-to-sequence translation.)\n","* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7bdZWxvJrsx","executionInfo":{"status":"ok","timestamp":1665890573546,"user_tz":360,"elapsed":12384,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"7db19416-ca48-4659-cfbc-6b95efcd402f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-16 03:22:38--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n","Resolving piazza.com (piazza.com)... 18.215.228.240, 52.200.164.228, 18.207.91.3, ...\n","Connecting to piazza.com (piazza.com)|18.215.228.240|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n","--2022-10-16 03:22:39--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n","Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.226.228.49, 13.226.228.46, 13.226.228.50, ...\n","Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.226.228.49|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1533290 (1.5M) [application/x-gzip]\n","Saving to: ‘./text_files.tar.gz’\n","\n","./text_files.tar.gz 100%[===================>]   1.46M  2.04MB/s    in 0.7s    \n","\n","2022-10-16 03:22:40 (2.04 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","file_len = 2579888\n"]}],"source":["! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n","! tar -xzf text_files.tar.gz\n","! pip install unidecode\n","! pip install torch\n","\n","import unidecode\n","import string\n","import random\n","import re\n"," \n","import pdb\n"," \n","all_characters = string.printable\n","n_characters = len(all_characters)\n","file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxBeKeNjJ0NQ","executionInfo":{"status":"ok","timestamp":1665890581480,"user_tz":360,"elapsed":312,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"07054ed8-7add-4faf-bfe2-cc7b01de36cb"},"outputs":[{"output_type":"stream","name":"stdout","text":[" great wizard than Gandalf did, \n","more kingly, beautiful, and powerful; and older. Yet by a sense other than \n","sight Pippin perceived that Gandalf had the greater power and the deeper \n","wisdom, and a maje\n"]}],"source":["chunk_len = 200\n"," \n","def random_chunk():\n","  start_index = random.randint(0, file_len - chunk_len)\n","  end_index = start_index + chunk_len + 1\n","  return file[start_index:end_index]\n","  \n","print(random_chunk())"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On0_WitWJ99e","executionInfo":{"status":"ok","timestamp":1665890588314,"user_tz":360,"elapsed":2882,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"063508c5-b78e-495b-864f-3a6065f67944"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10, 11, 12, 39, 40, 41])\n"]}],"source":["import torch\n","# Turn string into list of longs\n","def char_tensor(string):\n","  tensor = torch.zeros(len(string)).long()\n","  for c in range(len(string)):\n","      tensor[c] = all_characters.index(string[c])\n","  return tensor\n","\n","print(char_tensor('abcDEF'))"]},{"cell_type":"markdown","metadata":{"id":"CYJPTLcaYmfI"},"source":["---\n","\n","## Part 4: Creating your own GRU cell \n","\n","**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n","\n","---\n","\n","The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n","\n","Please do not look at the documentation's code for the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n","\n","**TODO:**\n","* Create a custom GRU cell\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"aavAv50ZKQ-F","executionInfo":{"status":"ok","timestamp":1665890851795,"user_tz":360,"elapsed":307,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GRU(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(GRU, self).__init__()\n","    self.n_layers = num_layers\n","    self.hz = []\n","    self.inn = []\n","    self.hn = []\n","    self.ir = []\n","    self.hr = []\n","    self.iz = []\n","    \n","    for i in range(self.n_layers):\n","      self.hz.append(nn.Linear(hidden_size, hidden_size))\n","      self.inn.append(nn.Linear(input_size,  hidden_size))\n","      self.hn.append(nn.Linear(hidden_size, hidden_size))\n","      self.ir.append(nn.Linear(input_size,  hidden_size))\n","      self.hr.append(nn.Linear(hidden_size, hidden_size))\n","      self.iz.append(nn.Linear(input_size,  hidden_size))\n","      \n","    \n","  \n","  def forward(self, inputs, hidden):\n","    # Each layer does the following:\n","    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n","    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n","    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n","    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n","    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n","    for i in range(self.n_layers):\n","      r_t = nn.Sigmoid(self.ir[i](inputs) + self.hr[i](hidden))\n","      z_t = nn.Sigmoid(self.iz[i](inputs) + self.hz[i](hidden))\n","      n_t = nn.Tanh(self.inn[i](inputs) + r_t*self.hn[i](hidden))\n","      hidden = (1 - z_t) * n_t + z_t * hidden\n","      output = hidden[-1:]\n","\n","    \n","    return output, hidden\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"qtXdX-B_WiAY"},"source":["---\n","\n","##  Part 1: Building a sequence to sequence model\n","\n","---\n","\n","Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n","\n","We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n","\n","\n","**TODO:**\n","* Create an RNN class that extends from nn.Module.\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"d6tNdEnzWj5F","executionInfo":{"status":"ok","timestamp":1665890796895,"user_tz":360,"elapsed":10,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","    super(RNN, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.n_layers = n_layers\n","    self.embedding = nn.Embedding(input_size, hidden_size)\n","    self.gru = nn.GRU(hidden_size, hidden_size, num_layers=self.n_layers)\n","    self.lin = nn.Linear(hidden_size, output_size)\n","\n","  def forward(self, input_char, hidden):\n","    # by reviewing the documentation, construct a forward function that properly uses the output\n","    # of the GRU\n","    output = self.embedding(input_char).view(1, 1, -1)\n","    output, hidden = self.gru(output, hidden)\n","    output = F.relu(output)\n","    output = self.lin(output)\n","    return output, hidden\n","\n","  def init_hidden(self):\n","    return torch.zeros(self.n_layers, 1, self.hidden_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"hrhXghEPKD-5","executionInfo":{"status":"ok","timestamp":1665890732783,"user_tz":360,"elapsed":293,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["def random_training_set():    \n","  chunk = random_chunk()\n","  inp = char_tensor(chunk[:-1])\n","  target = char_tensor(chunk[1:])\n","  return inp, target"]},{"cell_type":"markdown","metadata":{"id":"ZpiGObbBX0Mr"},"source":["---\n","\n","## Part 2: Sample text and Training information\n","\n","---\n","\n","We now want to be able to train our network, and sample text after training.\n","\n","This function outlines how training a sequence style network goes. \n","\n","**TODO:**\n","* Fill in the pieces.\n","\n","**DONE:**\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"2ALC3Pf8Kbsi","executionInfo":{"status":"ok","timestamp":1665890856068,"user_tz":360,"elapsed":334,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n","def train(input, target):\n","  ## initialize hidden layers, set up gradient and loss \n","    # your code here\n","  ## /\n","  decoder_optimizer.zero_grad()\n","  hidden = decoder.init_hidden()\n","  loss = 0\n","  for i, tar in zip(input, target):\n","    char, hidden = decoder(i, hidden)\n","    tar = tar.unsqueeze(0)\n","    loss += criterion(char.squeeze(0), tar)\n","  loss.backward()\n","  decoder_optimizer.step()\n","  return loss.item()/len(input)\n","    "]},{"cell_type":"markdown","metadata":{"id":"EN06NUu3YRlz"},"source":["---\n","\n","## Part 3: Sample text and Training information\n","\n","---\n","\n","You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n","\n","If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n","\n","**TODO:**\n","* Fill out the evaluate function to generate text frome a primed string\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"B-bp-OZ1KjNh","executionInfo":{"status":"ok","timestamp":1665890801001,"user_tz":360,"elapsed":297,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["def sample_outputs(output, temperature):\n","    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n","    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n","    # As temperature approaches infinity, this sampling function becomes a purely random choice\n","    return torch.multinomial(torch.exp(output / temperature), 1)\n","\n","def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n","  ## initialize hidden state, initialize other useful variables\n","    # your code here\n","  ## /\n","  hidden = decoder.init_hidden()\n","  prediction = prime_str + ''\n","  primer = char_tensor(prime_str) \n","  for char in primer[:-1]:\n","    x, hidden = decoder(char, hidden)\n","  last_char = primer[-1]\n","  \n","  for i in range(predict_len):\n","    pred, hidden = decoder(last_char, hidden)\n","    index = sample_outputs(pred.squeeze(0), temperature)\n","    decoded = all_characters[index]\n","    last_char = char_tensor(decoded)\n","    prediction += decoded\n","      \n","  return prediction"]},{"cell_type":"markdown","metadata":{"id":"Du4AGA8PcFEW"},"source":["---\n","\n","## Part 4: (Create a GRU cell, requirements above)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"GFS2bpHSZEU6"},"source":["\n","---\n","\n","## Part 5: Run it and generate some text!\n","\n","---\n","\n","\n","**TODO:** \n","* Create some cool output\n","\n","**DONE:**\n","\n","\n","\n","\n","Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n","\n","---\n","\n"," G:\n"," \n"," Gandalf was decrond. \n","'All have lord you. Forward the road at least walk this is stuff, and \n","went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n","row hrough. In  \n","\n"," lo:\n"," \n"," lost death it. \n","'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n","'Yes, as you there were remembaused to seen their pass, when? What \n","said here, such seven an the sear \n","\n"," lo:\n"," \n"," low, and frod to keepn \n","Came of their most. But here priced doubtless to an Sam up is \n","masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n","the like \n","\n"," I:\n"," \n"," I had been the \n","in his eyes with the perushed to lest, if then only the ring and the legended \n","of the less of the long they which as the \n","enders of Orcovered and smood, and the p \n","\n"," I:\n"," \n"," I they were not the lord of the hoomes. \n","Home already well from the Elves. And he sat strength, and we \n","housed out of the good of the days to the mountains from his perith. \n","\n","'Yess! Where though as if  \n","\n"," Th:\n"," \n"," There yarden \n","you would guard the hoor might. Far and then may was \n","croties, too began to see the drumbred many line \n","and was then hoard walk and they heart, and the chair of the \n","Ents of way, might was \n","\n"," G:\n"," \n"," Gandalf \n","been lat of less the round of the stump; both and seemed to the trees and perished they \n","lay are speered the less; and the wind the steep and have to she \n","precious. There was in the oonly went \n","\n"," wh:\n"," \n"," which went out of the door. \n","Hull the King and of the The days of his brodo \n","stumbler of the windard was a thing there, then it been shining langing \n","to him poor land. They hands; though they seemed ou \n","\n"," ra:\n"," \n"," rather,' have all the least deather \n","down of the truven beginning to the house of sunk. \n","'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n","'I wander trust horn, and there were not, it  \n","\n"," I:\n"," \n"," I can have no mind \n","together! Where don't may had one may little blung \n","terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n","not stopped great the out them forms. On they she lo \n","\n","---\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"-nXFeCmdKodw","executionInfo":{"status":"ok","timestamp":1665890806961,"user_tz":360,"elapsed":316,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}}},"outputs":[],"source":["import time\n","n_epochs = 2000\n","print_every = 200\n","plot_every = 10\n","hidden_size = 200\n","n_layers = 3\n","lr = 0.001\n"," \n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n"," \n","start = time.time()\n","all_losses = []\n","loss_avg = 0"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"xKfozqw-6eqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665892166178,"user_tz":360,"elapsed":1305451,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"913cf33f-3b48-4e19-d823-cf259f1d6724"},"outputs":[{"output_type":"stream","name":"stdout","text":["[190.60812306404114 (200 10%) 2.8546]\n","Wh e d mhml htt seeehr iH le  hdld, ittadiae nn aftinafmneh ,adnt eeoiiaee eih a y, uhr s' smasa e n n \n","\n","[321.4662744998932 (400 20%) 2.6009]\n","Wh  arnirt nome -afrdeur atln noh nwie tdolle oboee eet es waor Rbsa oe noe \n","aimee ond nod hue ah rh.e \n","\n","[450.3840799331665 (600 30%) 2.5767]\n","Whe herge ceoivesdhen hte anuel on she covy Enn fosmoth. undAs ent whroas to faos. uore weoath he ders \n","\n","[578.66535115242 (800 40%) 2.1709]\n","Whis and the penger stould the an free. sind drorred fow ward dowttet tento Fhver rind had wearn \n","teri \n","\n","[709.0300748348236 (1000 50%) 1.9727]\n","Whing, said \n","\n","Gandring bado. 'But but Sam the Hips as cardogold and hidden paleding beth taden, and th \n","\n","[838.7990665435791 (1200 60%) 1.8089]\n","Where \n","leat it,s eyes. It a light of the metun from lieaden stailed \n","it yelike to air. I tried as and  \n","\n","[967.4614770412445 (1400 70%) 1.7571]\n","Wh? 'He hime se-gent wordden it the \n","wicomks hinlis,' sho a seel warkly be as we he clime and passey,  \n","\n","[1098.2493064403534 (1600 80%) 1.6979]\n","Whhand sear leadd head. Numvil they here and End the stilk looked, and seen and sscoucaseed thereing e \n","\n","[1227.8962469100952 (1800 90%) 1.7574]\n","Where the \n","stimes thut he straster head. \n","\n","Then before the mountains of the battiw the. The \n","did. 'He  \n","\n","[1358.9730806350708 (2000 100%) 1.7260]\n","Wh that cur, Frodom \n","upon the kere of the shile into the hilents that beards their great betters of th \n","\n"]}],"source":["# n_epochs = 2000\n","for epoch in range(1, n_epochs + 1):\n","  loss_ = train(*random_training_set())       \n","  loss_avg += loss_\n","\n","  if epoch % print_every == 0:\n","      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","      print(evaluate('Wh', 100), '\\n')\n","\n","  if epoch % plot_every == 0:\n","      all_losses.append(loss_avg / plot_every)\n","      loss_avg = 0"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ee0so6aKJ5L8","executionInfo":{"status":"ok","timestamp":1665892189912,"user_tz":360,"elapsed":2234,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"b37e9be1-b624-42d6-b290-7f3646dd599b"},"outputs":[{"output_type":"stream","name":"stdout","text":[" I \n"," I had road. He had now that which I silent some, now he hacmed, and it \n","far be \n","all some them leading it wish when you Andol, the arvall the door it them. \n","\n","'I walking of there weth yet \n","For the stood e \n","\n"," lo\n"," long for \n","many from betward him Load it the greep the \n","shall host his south the chout water \n","lealling could there goed in thh ham the bagn and a dapder do flecied deep of the last and the hoads at befor \n","\n"," G\n"," Gines men no paliping to hiys and \n","sursed the most of the shagotted midder betwawn his back the passed for will sterd. The lam, and be should coning Scame wonder the howly that you pers which that meen \n","\n"," lo\n"," long tom the right that stronger. 'He pook we arts of southy never uping who stided in the oft. Black then a was hall,' said Light to the felt on the for \n","atted it icling \n","before come the for the words  \n","\n"," G\n"," Gandalf with story your splak at Gafly. Though cra one to the gloor of he have lay, and that the house him. Frodo have have sorched time uncil and they rouve sadded about come the sdided \n","for the \n","name \n","\n"," G\n"," Gindoot the mi windoor for all when lead the duntill the hage Gandalf of the sname and peace that streen it high the El. Eet have tive all the storn his alones of an archir or betwall, Merry beor the h \n","\n"," lo\n"," loud of the \n","Hollous. 'He made for them not meant \n","to the find for it in his hour the Mroon. But great of shadou tell in the stood shellad the wards had pilled moiting as his to the shaling and of hemce \n","\n"," ca\n"," can at a that is noithure against they was sayded. \n","\n","Frodo. \n","\n","'The chamforting to his south and the morched his any namer. You dees hooped the ron would finder of the are wey silept \n","clead thereing in t \n","\n"," G\n"," Gind and finder the wandalf. \n","\n","There land to poler could he shed, pelless evenly. \n","\n","'I lead it did have duad trough alfor our to bept the I tourn well it the foos,' he fell and \n","amid, and a old. \n","\n","'Yap \n","\n"," he\n"," he were it has said Gandall, in the ars with them landed the seep. And it we him was creach and time \n","were the vair of them wou Iser it be went did \n","up thrat yes of \n","the doip wear: though have came they \n","\n"]}],"source":["for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"]},{"cell_type":"markdown","metadata":{"id":"YJhgDc2IauPE"},"source":["---\n","\n","## Part 6: Generate output on a different dataset\n","\n","---\n","\n","**TODO:**\n","\n","* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n","\n","* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","source":["file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)\n","\n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","\n","start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(n_epochs):\n","  loss_ = train(*random_training_set())       \n","  loss_avg += loss_\n","\n","  if epoch % print_every == 0:\n","    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","    print(evaluate('Wh', predict_len=100, temperature=0.6), '\\n')\n","\n","  if epoch % plot_every == 0:\n","    all_losses.append(loss_avg / plot_every)\n","    loss_avg = 0"],"metadata":{"id":"Z_BLpgyQXURZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WxHCuJQzgzrS","executionInfo":{"status":"ok","timestamp":1665708922633,"user_tz":360,"elapsed":1958,"user":{"displayName":"Tade Davis","userId":"14229488086641410854"}},"outputId":"4140d81d-07f4-4462-ede1-5937a1888885"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" ca\n"," can suce conto.\n","\n","LADWICS:\n","I, shall is gailost his be coming thew mistain,\n","As this and Poss, and sees love in long intis turn'd to\n","hould the blood could be and there of that fear in thef\n","the say he gook  \n","\n"," he\n"," her gods hrow.\n","Have thee, and then the grace.\n","\n","BIBTASTAR:\n","Yon? we with in my see, wing gone bake, come,\n","That this a pits with one peass.\n","\n","DUKEU:\n","Vaintite dober more love a colcoke her a croped,\n","\n","BUCCILA \n","\n"," wh\n"," whom you had then we\n","God, by but me ire forst, this being nend,\n","Whither with my spaw in sown wair you hadst your kindre us must not some,\n","By sceeus the roon.\n","\n","FRILGO ENIO:\n","I pass so her! wonk, Tranaty a \n","\n"," I \n"," I and the must they being me.\n","\n","LUCIO:\n","My live!\n","\n","LEONDA:\n","But that Gurst, ristess werion and my had?\n","\n","MENENANSUONE:\n","Nor, in hand mady the king will dead you real'd,\n","And this forful contery thou form'd wer \n","\n"," Th\n"," Thath met all well,\n","A modannand all in protor and that state; this indein, be gone:\n","Most and me am on that hold of he ruse tomebher appus.\n","\n","Stree:\n","Where you to stand and to some with binds\n","The can cay s \n","\n"," Th\n"," Thatting away of jouls,\n","Thinss well that speak'd this earth s ampers to with never so.\n","\n","MENENIUS:\n","Whom you you for with the nost larch in thad at Dut beom.\n","I'll is lo serp enfet to the untiness tore.\n","\n","D \n","\n"," wh\n"," what stands bist:\n","Of the tems of his to prile ray, moin? be? briar'd us them me and withhell.\n","Ame, I say to forst day to my triince\n","Tto shear thank Rome thy call'd end hid reven;\n","What thy done speaced o \n","\n"," G\n"," Gates,\n","Thy corn to death, be Have of worth,\n","A saat, and this works and nate have and you,\n","This have born not at thy brand your grastion?\n","A stake of eegly you grast the shave with be power they from out \n","\n"," ca\n"," came and my mow\n","and me gardon on the nive the are us-them's thind Listion with with be of Perst.\n","\n","MENENIONENO:\n","He speak tramon by frept on or been of me of you, they in fape.\n","\n","ROMEO:\n","I teres a nother:\n","Y \n","\n"," wh\n"," what be would\n","Which all to that dear a supple;\n","Come, the such this king enous to shall shorse have my triony:\n","Will you, for some I good the worst resers to my lord!\n","But you is for hem both of strow wist \n","\n"]}]},{"cell_type":"markdown","source":["Positives:\n","\n","*   It generally was able to generate full words\n","*   A lot of the text actually sounds Shakespearean\n","\n","Negatives:\n","\n","\n","*  The sentences are almost always unintelligible( then again so are most shakespearean texts)\n","*  It throws new lines in often when they shouldn't be there\n","\n"],"metadata":{"id":"bmvB3En5T3Ey"}}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}